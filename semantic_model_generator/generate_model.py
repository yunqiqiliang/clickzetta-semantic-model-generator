import os
import re
import math
from collections import defaultdict
from datetime import datetime
from typing import Any, Callable, Dict, List, Optional, Tuple

from clickzetta.zettapark.session import Session
from loguru import logger

from semantic_model_generator.data_processing import data_types, proto_utils
from semantic_model_generator.protos import semantic_model_pb2
from semantic_model_generator.clickzetta_utils.clickzetta_connector import (
    AUTOGEN_TOKEN,
    DIMENSION_DATATYPES,
    MEASURE_DATATYPES,
    OBJECT_DATATYPES,
    TIME_MEASURE_DATATYPES,
    get_table_representation,
    get_valid_schemas_tables_columns_df,
)
from semantic_model_generator.clickzetta_utils.utils import create_fqn_table
from semantic_model_generator.validate.context_length import validate_context_length
from semantic_model_generator.llm import (
    DashscopeClient,
    DashscopeSettings,
    enrich_semantic_model,
    get_dashscope_settings,
)
from semantic_model_generator.validate.keywords import CZ_RESERVED_WORDS

_PLACEHOLDER_COMMENT = "  "
_FILL_OUT_TOKEN = " # <FILL-OUT>"
# TODO add _AUTO_GEN_TOKEN to the end of the auto generated descriptions.
_AUTOGEN_COMMENT_TOKEN = (
    " # <AUTO-GENERATED DESCRIPTION, PLEASE MODIFY AND REMOVE THE __ AT THE END>"
)
_DEFAULT_N_SAMPLE_VALUES_PER_COL = 10
_AUTOGEN_COMMENT_WARNING = f"# NOTE: This file was auto-generated by the semantic model generator. Please fill out placeholders marked with {_FILL_OUT_TOKEN} (or remove if not relevant) and verify autogenerated comments.\n"

def _singularize(token: str) -> str:
    if token.endswith("IES") and len(token) > 3:
        return token[:-3] + "Y"
    if token.endswith(("SES", "XES", "ZES", "CHES", "SHES")) and len(token) > 4:
        return token[:-2]
    if token.endswith("S") and len(token) > 3:
        return token[:-1]
    return token


def _clean_column_type(raw: Any) -> str:
    if raw is None:
        return ""
    text = str(raw).strip()
    if not text:
        return ""
    text = re.sub(r"\s+", " ", text.upper())
    for suffix in (" NOT NULL", " NULL"):
        if text.endswith(suffix):
            text = text[: -len(suffix)].strip()
    return text


def _base_type_from_type(column_type: str) -> str:
    cleaned = _clean_column_type(column_type)
    token = cleaned.split(" ")[0] if cleaned else ""
    return token.split("(")[0]


def _identifier_tokens(name: str, prefixes_to_drop: Optional[set[str]] = None) -> List[str]:
    name = name.replace("-", "_")
    raw_tokens = re.split(r"[^0-9A-Za-z]+", name)
    tokens: List[str] = []
    for token in raw_tokens:
        if not token:
            continue
        split = re.sub(r"([a-z0-9])([A-Z])", r"\1 \2", token).split()
        for part in split:
            tokens.append(part.upper())
    tokens = [token for token in tokens if token]
    if prefixes_to_drop and len(tokens) >= 2 and tokens[0] in prefixes_to_drop:
        tokens = tokens[1:]
    return tokens


def _sanitize_identifier_name(name: str, prefixes_to_drop: Optional[set[str]] = None) -> str:
    if not name:
        return ""

    cleaned = name.replace("-", "_")
    cleaned = re.sub(r"\s+", "_", cleaned)
    parts = [part for part in cleaned.split("_") if part]
    if not parts:
        return ""

    if len(parts) >= 2 and len(parts[0]) == 1:
        parts = parts[1:]

    if prefixes_to_drop:
        drop_tokens = {token.upper() for token in prefixes_to_drop}
        while parts and parts[0].upper() in drop_tokens:
            parts = parts[1:]

    if not parts:
        return ""

    rebuilt = "_".join(parts)
    tokens = _identifier_tokens(rebuilt)
    if not tokens:
        return ""

    if len(parts) == 1:
        return "".join(tokens)
    return "_".join(tokens)


_INVALID_ID_CHARS = re.compile(r"[^0-9A-Za-z_$]")


def _is_valid_identifier(name: str) -> bool:
    if not name:
        return False
    candidate = name.strip()
    if not candidate:
        return False
    return candidate.replace("_", "").replace("$", "").isalnum()


def _normalize_identifier(name: str) -> str:
    if not name:
        return ""
    text = re.sub(r"\s+", "_", name.strip())
    text = text.replace("-", "_")
    text = _INVALID_ID_CHARS.sub("", text)
    text = re.sub(r"_+", "_", text)
    return text.strip("_")


def _safe_semantic_identifier(
    raw_name: str,
    used_names: set[str],
    fallback_prefix: str,
    prefixes_to_drop: Optional[set[str]] = None,
) -> str:
    """
    Produce a YAML-safe identifier that avoids reserved keywords and duplicates.
    """

    variants: List[str] = []
    normalized = _normalize_identifier(raw_name)
    if normalized:
        variants.append(normalized)
    sanitized = _sanitize_identifier_name(raw_name, prefixes_to_drop=prefixes_to_drop)
    if sanitized:
        variants.append(sanitized.lower())
    variants.append(f"{fallback_prefix}_field")

    for variant in variants:
        candidate = _normalize_identifier(variant)
        if not candidate:
            continue
        if candidate[0].isdigit():
            candidate = f"{fallback_prefix}_{candidate}"
        candidate = candidate.lower()
        base = candidate
        suffix = 2
        while (
            base in used_names
            or base.upper() in CZ_RESERVED_WORDS
            or not _is_valid_identifier(base)
        ):
            base = f"{candidate}_{suffix}"
            suffix += 1
        if base.upper() in CZ_RESERVED_WORDS or not _is_valid_identifier(base):
            continue
        used_names.add(base)
        return base

    suffix = 1
    while True:
        candidate = f"{fallback_prefix}_{suffix}"
        if (
            candidate not in used_names
            and candidate.upper() not in CZ_RESERVED_WORDS
            and _is_valid_identifier(candidate)
        ):
            used_names.add(candidate)
            return candidate
        suffix += 1


def _is_identifier_like(column_name: str, base_type: str) -> bool:
    """
    Heuristic to detect identifier-style numeric columns that should stay as dimensions.
    """

    tokens = _identifier_tokens(column_name)
    if not tokens:
        return False

    tail = tokens[-1]
    if tail in {"ID", "KEY", "ROWID", "ROW_ID", "PK", "PRIMARY", "PRIMARYKEY"}:
        return True
    if tail.endswith("ID") and len(tail) > 2:
        return True
    if tail.endswith("KEY") and len(tail) > 3:
        return True

    return False


def _table_variants(table_name: str) -> set[str]:
    table_upper = table_name.upper()
    tokens = _identifier_tokens(table_name)
    variants = {table_upper}
    variants.update(tokens)
    for token in list(variants):
        variants.add(_singularize(token))
        if len(token) > 3:
            variants.add(token[:4])
        if len(token) > 3:
            variants.add(token[:3])
        if len(token) > 3:
            variants.add(token[-4:])
        if len(token) > 2:
            variants.add(token[-3:])
    return {variant for variant in variants if variant}


_GENERIC_PREFIXES = {
    "DIM",
    "FACT",
    "FCT",
    "BRIDGE",
    "BRG",
    "STG",
    "ODS",
    "OD",
    "DW",
    "VW",
    "VIEW",
    "HUB",
    "SAT",
    "LNK",
    "TMP",
    "TMPV",
}


def _table_prefixes(table_name: str) -> set[str]:
    prefixes: set[str] = set()
    tokens = _identifier_tokens(table_name)
    if not tokens:
        tokens = [table_name.upper()]

    first_token = tokens[0].upper()
    if first_token in _GENERIC_PREFIXES:
        for length in range(1, min(len(first_token), 4) + 1):
            prefixes.add(first_token[:length])
        prefixes.add(first_token)
    return prefixes


def _looks_like_primary_key(table_name: str, column_name: str) -> bool:
    upper_name = column_name.strip().upper()
    variants = _table_variants(table_name)
    direct_matches = {
        "ID",
        "PK",
        "PRIMARY_KEY",
    }
    for variant in variants:
        direct_matches.update({f"{variant}_ID", f"{variant}ID", f"{variant}_KEY", f"{variant}KEY"})
    if upper_name in direct_matches:
        return True

    tokens = _identifier_tokens(column_name)
    if not tokens:
        return False
    last = tokens[-1]
    if last in {"ID", "KEY"} and len(tokens) >= 2 and tokens[-2] in variants:
        return True
    if last.endswith("ID") and last[:-2] in variants:
        return True
    if last.endswith("KEY") and last[:-3] in variants:
        return True
    if len(tokens) == 1 and last.endswith(("ID", "KEY")):
        stem = last[:-2] if last.endswith("ID") else last[:-3]
        if stem in variants:
            return True

    return False


_TIME_NAME_HINTS = ("DATE", "TIME", "TIMESTAMP", "DT", "DAY", "HOUR")
_ISO_DATE_REGEX = re.compile(r"^\d{4}[-/]\d{2}[-/]\d{2}")


def _is_time_like_column(column: data_types.Column) -> bool:
    base_type = _base_type_from_type(column.column_type)
    if base_type in TIME_MEASURE_DATATYPES:
        return True
    if base_type not in {"STRING", "VARCHAR", "TEXT", "CHAR", "CHARACTER", "NVARCHAR"}:
        return False

    column_name = column.column_name.upper()
    if any(hint in column_name for hint in _TIME_NAME_HINTS):
        return True

    for value in (column.values or [])[:5]:
        val = str(value).strip()
        if not val:
            continue
        candidate = val
        if candidate.endswith("Z"):
            candidate = candidate[:-1]
        candidate = candidate.replace(" ", "T", 1)
        try:
            datetime.fromisoformat(candidate)
            return True
        except ValueError:
            pass
        if _ISO_DATE_REGEX.match(val):
            return True
    return False


def _format_literal(value: str, base_type: str) -> str:
    text = "" if value is None else str(value)
    if base_type in MEASURE_DATATYPES:
        try:
            if "." in text:
                return str(float(text))
            return str(int(text))
        except ValueError:
            pass
    lowered = text.lower()
    if lowered in {"true", "false"}:
        return lowered.upper()
    escaped = text.replace("'", "''")
    return f"'{escaped}'"


def _format_sql_identifier(name: str) -> str:
    """
    Formats an identifier for SQL (without quoting) by stripping quotes and uppercasing.
    """
    if not name:
        return ""
    return str(name).replace('"', "").replace("`", "").strip().upper()


def _qualified_table_name(fqn: data_types.FQNParts) -> str:
    """
    Builds a fully qualified table name without quoting.
    """
    parts = [part for part in (fqn.database, fqn.schema_name, fqn.table) if part]
    return ".".join(_format_sql_identifier(part) for part in parts if part)


def _levenshtein_distance(s1: str, s2: str) -> int:
    """
    Calculate Levenshtein distance between two strings.
    Used for fuzzy column name matching.
    """
    if len(s1) < len(s2):
        return _levenshtein_distance(s2, s1)
    if len(s2) == 0:
        return len(s1)
    
    previous_row = range(len(s2) + 1)
    for i, c1 in enumerate(s1):
        current_row = [i + 1]
        for j, c2 in enumerate(s2):
            insertions = previous_row[j + 1] + 1
            deletions = current_row[j] + 1
            substitutions = previous_row[j] + (c1 != c2)
            current_row.append(min(insertions, deletions, substitutions))
        previous_row = current_row
    
    return previous_row[-1]


def _name_similarity(name1: str, name2: str) -> float:
    """
    Calculate normalized similarity score between two column names.
    Returns a score between 0.0 (completely different) and 1.0 (identical).
    """
    if not name1 or not name2:
        return 0.0
    
    # Exact match
    if name1.upper() == name2.upper():
        return 1.0
    
    # Normalize names for comparison
    norm1 = name1.upper().replace("_", "").replace("-", "")
    norm2 = name2.upper().replace("_", "").replace("-", "")
    
    if norm1 == norm2:
        return 0.95
    
    # Calculate Levenshtein-based similarity
    max_len = max(len(norm1), len(norm2))
    if max_len == 0:
        return 0.0
    
    distance = _levenshtein_distance(norm1, norm2)
    similarity = 1.0 - (distance / max_len)
    
    return max(0.0, similarity)


def _infer_cardinality(
    left_values: List[str],
    right_values: List[str],
    left_is_pk: bool,
    right_is_pk: bool,
) -> tuple[str, str]:
    """
    Infer relationship cardinality based on uniqueness ratios.
    
    Returns:
        tuple: (left_cardinality, right_cardinality) where values can be:
               "*" (many), "1" (one), "?" (zero or one), "+" (one or more)
    
    Note:
        Only trusts uniqueness heuristics when we have sufficient sample size (>= 50)
        to avoid false positives. With small samples (e.g., 10 rows), a fact table's
        foreign key can appear 100% unique by chance, leading to incorrect 1:1 inference.
    """
    # RULE 1: Trust explicit primary key metadata (highest priority)
    if right_is_pk and not left_is_pk:
        return ("*", "1")
    
    if left_is_pk and not right_is_pk:
        return ("1", "*")
    
    if left_is_pk and right_is_pk:
        return ("1", "1")
    
    # RULE 2: Only use uniqueness heuristics with sufficient sample size
    # Require at least 50 samples to trust uniqueness ratios
    MIN_SAMPLE_SIZE = 50
    
    if left_values and right_values:
        sample_size = min(len(left_values), len(right_values))
        
        # Small sample warning: fall back to safe default
        if sample_size < MIN_SAMPLE_SIZE:
            # Conservative default: assume many-to-one (most common pattern)
            # This prevents false 1:1 inference from small samples
            return ("*", "1")
        
        # Large enough sample: trust uniqueness ratios
        left_unique_ratio = len(set(left_values)) / len(left_values) if left_values else 0
        right_unique_ratio = len(set(right_values)) / len(right_values) if right_values else 0
        
        # High uniqueness (>0.95) suggests the column could be a key
        left_is_unique = left_unique_ratio > 0.95
        right_is_unique = right_unique_ratio > 0.95
        
        if left_is_unique and right_is_unique:
            return ("1", "1")
        elif right_is_unique:
            return ("*", "1")
        elif left_is_unique:
            return ("1", "*")
    
    # RULE 3: Default to many-to-one (most common case in data warehouses)
    return ("*", "1")


def _is_nullish(value: Any) -> bool:
    """
    Determines whether a sampled value should be treated as NULL/empty.
    Handles common ClickZetta sampling formats where NULL becomes NaN or an empty string.
    """
    if value is None:
        return True
    if isinstance(value, float) and math.isnan(value):
        return True
    text = str(value).strip()
    if not text:
        return True
    normalized = text.upper()
    return normalized in {"NULL", "NONE", "NAN", "NA", "N/A", "NOT AVAILABLE"}


def _column_has_null_via_query(
    session: Session,
    fqn: data_types.FQNParts,
    column_name: str,
    cache: Dict[Tuple[str, str, str, str], bool],
) -> bool:
    """
    Executes a targeted IS NULL query to conclusively detect nullable foreign keys.
    Results are cached per table/column to avoid redundant lookups.
    """
    key = (
        (fqn.database or "").upper(),
        (fqn.schema_name or "").upper(),
        (fqn.table or "").upper(),
        column_name.upper(),
    )
    if key in cache:
        return cache[key]

    if not column_name:
        cache[key] = False
        return False

    qualified_table = _qualified_table_name(fqn)
    column_identifier = _format_sql_identifier(column_name)
    if not column_identifier or not qualified_table:
        cache[key] = False
        return False

    query = (
        f"SELECT {column_identifier} FROM {qualified_table} "
        f"WHERE {column_identifier} IS NULL LIMIT 1"
    )

    try:
        df = session.sql(query).to_pandas()
        has_null = not df.empty
    except Exception as exc:  # pragma: no cover - best-effort diagnostic query
        logger.debug(
            "Strict null check query failed for %s.%s.%s.%s: %s",
            fqn.database,
            fqn.schema_name,
            fqn.table,
            column_name,
            exc,
        )
        has_null = False

    cache[key] = has_null
    return has_null


def _infer_join_type(
    left_table: str,
    right_table: str,
    left_card: str,
    right_card: str,
    left_is_pk: bool,
    right_is_pk: bool,
    left_values: List[str],
    right_values: List[str],
    *,
    has_null_fk: bool = False,
) -> int:
    """
    Infer the appropriate JOIN type for a relationship.
    
    Args:
        left_table: Name of the left table
        right_table: Name of the right table
        left_card: Left cardinality ("1", "*", etc.)
        right_card: Right cardinality ("1", "*", etc.)
        left_is_pk: Whether left column is a primary key
        right_is_pk: Whether right column is a primary key
        left_values: Sample values from left column
        right_values: Sample values from right column
    
    Returns:
        semantic_model_pb2.JoinType value (inner or left_outer)
    
    Decision Logic:
        1. INNER JOIN (default): When both sides have matching records
           - Standard FK → PK relationships
           - Both sides are required (e.g., orders must have customers)
        
        2. LEFT OUTER JOIN: When left side may have orphaned records
           - Nullable foreign keys (detected via NULL sample values)
           - Optional relationships (e.g., orders.promo_code → promotions.code)
           - Fact → Dimension with potential missing dimensions
    
    Note:
        - RIGHT OUTER and FULL OUTER are deprecated and not used
        - We prefer LEFT OUTER over RIGHT OUTER for readability
        - INNER JOIN is the safe default for data integrity
    """
    
    # RULE 1: Default to INNER JOIN (most common and safest)
    # Ensures referential integrity and matches standard data warehouse practice
    default_join = semantic_model_pb2.JoinType.inner
    
    # RULE 2: Strict mode override - explicit null detection via SQL
    if has_null_fk:
        reason = "strict null probe"
        logger.debug(
            f"Join type inference for {left_table} -> {right_table}: "
            f"LEFT_OUTER ({reason})"
        )
        return semantic_model_pb2.JoinType.left_outer

    # RULE 3: Check for NULL values in foreign key (left side) using samples
    # NULL values indicate optional relationships → use LEFT OUTER
    if left_values and left_card in ("*", "+"):
        sample_window = left_values[: min(len(left_values), 25)]
        has_nulls = any(_is_nullish(val) for val in sample_window)
        if has_nulls:
            logger.debug(
                f"Join type inference for {left_table} -> {right_table}: "
                f"LEFT_OUTER (detected NULL values in FK column)"
            )
            return semantic_model_pb2.JoinType.left_outer
    
    # RULE 4: Heuristic - Look for "optional" naming patterns
    # Tables/columns with "optional", "alternate", "secondary" suggest LEFT OUTER
    left_upper = left_table.upper()
    right_upper = right_table.upper()
    optional_keywords = {
        "OPTIONAL", "ALTERNATE", "SECONDARY", "BACKUP", "FALLBACK",
        "PROMO", "PROMOTION", "DISCOUNT", "COUPON",  # Often optional
    }
    
    for keyword in optional_keywords:
        if keyword in right_upper:
            logger.debug(
                f"Join type inference for {left_table} -> {right_table}: "
                f"LEFT_OUTER (optional relationship pattern: {keyword})"
            )
            return semantic_model_pb2.JoinType.left_outer
    
    # RULE 5: Many-to-One where dimension might be incomplete
    # Example: FACT_SALES → DIM_PRODUCT (some products might not exist yet)
    # However, this is risky - default to INNER for data quality
    # Users can manually override if needed
    
    # RULE 6: Default to INNER JOIN
    # Most conservative and ensures data integrity
    logger.debug(
        f"Join type inference for {left_table} -> {right_table}: "
        f"INNER (default - ensures referential integrity)"
    )
    return default_join


def _looks_like_foreign_key(fk_table: str, pk_table: str, fk_column: str) -> bool:
    """
    Enhanced heuristic to detect if a column looks like a foreign key.
    Checks patterns like: customer_id, cust_id, customer_key, etc.
    """
    fk_upper = fk_column.strip().upper()
    pk_table_variants = _table_variants(pk_table)
    
    # Pattern 1: {table_name}_id or {table_name}_key
    for variant in pk_table_variants:
        if fk_upper in {f"{variant}_ID", f"{variant}ID", f"{variant}_KEY", f"{variant}KEY"}:
            return True
    
    # Pattern 2: Column ends with table name variants
    tokens = _identifier_tokens(fk_column)
    if len(tokens) >= 2:
        # e.g., order_customer_id -> check if CUSTOMER is in pk_table_variants
        for i in range(len(tokens) - 1):
            if tokens[i] in pk_table_variants:
                tail = tokens[-1]
                if tail in {"ID", "KEY"}:
                    return True
    
    # Pattern 3: Similar to primary key column but with FK table prefix
    # e.g., order_id in order_items table referencing orders.id
    fk_table_variants = _table_variants(fk_table)
    for fk_variant in fk_table_variants:
        if fk_upper.startswith(fk_variant):
            remainder = fk_upper[len(fk_variant):].lstrip("_")
            for pk_variant in pk_table_variants:
                if remainder.startswith(pk_variant):
                    return True
    
    return False


def _suggest_filters(raw_table: data_types.Table) -> List[semantic_model_pb2.NamedFilter]:
    suggestions: List[semantic_model_pb2.NamedFilter] = []
    for col in raw_table.columns:
        base_type = _base_type_from_type(col.column_type)
        values = col.values or []
        distinct_values: List[str] = []
        for value in values:
            if value not in distinct_values:
                distinct_values.append(value)
        if _is_time_like_column(col):
            expr = f"{col.column_name} >= DATEADD('day', -30, CURRENT_DATE())"
            suggestions.append(
                semantic_model_pb2.NamedFilter(
                    name=f"{col.column_name}_last_30_days",
                    synonyms=[_PLACEHOLDER_COMMENT],
                    description=_PLACEHOLDER_COMMENT,
                    expr=expr,
                )
            )
        if 1 < len(distinct_values) <= 5:
            upper_name = col.column_name.upper()
            is_identifier_like = upper_name.endswith(("ID", "_ID", "KEY", "_KEY"))

            categorical_suffixes = (
                "STATUS",
                "FLAG",
                "TYPE",
                "PRIORITY",
                "SEGMENT",
                "CATEGORY",
                "MODE",
                "CODE",
                "LEVEL",
            )
            is_textual = base_type in {"STRING", "TEXT", "VARCHAR", "CHAR", "CHARACTER"}
            is_boolean = base_type in {"BOOLEAN"}
            is_categorical_numeric = base_type in {"INT", "INTEGER", "NUMBER", "SMALLINT", "BIGINT"} and any(
                upper_name.endswith(suffix) for suffix in categorical_suffixes
            )

            if not is_identifier_like and (is_textual or is_boolean or is_categorical_numeric):
                formatted = [_format_literal(val, base_type) for val in distinct_values[:5]]
                expr = f"{col.column_name} IN ({', '.join(formatted)})"
                suggestions.append(
                    semantic_model_pb2.NamedFilter(
                        name=f"{col.column_name}_include_values",
                        synonyms=[_PLACEHOLDER_COMMENT],
                        description=_PLACEHOLDER_COMMENT,
                        expr=expr,
                    )
                )
    return suggestions


def _infer_relationships(
    raw_tables: List[tuple[data_types.FQNParts, data_types.Table]],
    *,
    session: Optional[Session] = None,
    strict_join_inference: bool = False,
) -> List[semantic_model_pb2.Relationship]:
    relationships: List[semantic_model_pb2.Relationship] = []
    if not raw_tables:
        return relationships

    metadata = {}
    prefix_counter: Dict[str, int] = {}
    for _, raw_table in raw_tables:
        for column in raw_table.columns:
            tokens = _identifier_tokens(column.column_name)
            if tokens:
                prefix = tokens[0]
                prefix_counter[prefix] = prefix_counter.get(prefix, 0) + 1
    global_prefixes = {
        prefix
        for prefix, count in prefix_counter.items()
        if len(prefix) <= 3 and count >= 2
    }

    metadata = {}
    for fqn, raw_table in raw_tables:
        columns_meta: Dict[str, Dict[str, Any]] = {}
        pk_candidates: Dict[str, List[str]] = defaultdict(list)
        table_prefixes = global_prefixes | _table_prefixes(raw_table.name)
        for column in raw_table.columns:
            base_type = _base_type_from_type(column.column_type)
            normalized = _sanitize_identifier_name(column.column_name, prefixes_to_drop=table_prefixes)
            entry = columns_meta.setdefault(
                normalized,
                {
                    "names": [],
                    "base_type": base_type,
                    "values": [],
                    "is_identifier": False,
                    "is_primary": False,
                },
            )
            entry["base_type"] = base_type
            entry["names"].append(column.column_name)
            if column.values:
                entry["values"].extend(column.values)
            entry["is_identifier"] = entry["is_identifier"] or _is_identifier_like(column.column_name, base_type)
            is_primary = getattr(column, "is_primary_key", False)
            if is_primary:
                entry["is_primary"] = True
                if column.column_name not in pk_candidates[normalized]:
                    pk_candidates[normalized].append(column.column_name)
                continue
            if _looks_like_primary_key(raw_table.name, column.column_name):
                pk_candidates[normalized].append(column.column_name)
        metadata[raw_table.name] = {
            "fqn": fqn,
            "columns": columns_meta,
            "pk_candidates": pk_candidates,
        }

    pairs: dict[tuple[str, str], List[tuple[str, str]]] = {}
    null_check_cache: Dict[Tuple[str, str, str, str], bool] = {}

    def _record_pair(left_table: str, right_table: str, left_col: str, right_col: str) -> None:
        key = (left_table, right_table)
        value = (left_col, right_col)
        if value not in pairs.setdefault(key, []):
            pairs[key].append(value)

    table_names = list(metadata.keys())
    for i in range(len(table_names)):
        for j in range(i + 1, len(table_names)):
            table_a_name = table_names[i]
            table_b_name = table_names[j]
            table_a = metadata[table_a_name]
            table_b = metadata[table_b_name]

            shared = set(table_a["columns"].keys()) & set(table_b["columns"].keys())
            for col_key in shared:
                meta_a = table_a["columns"][col_key]
                meta_b = table_b["columns"][col_key]
                if meta_a["base_type"] != meta_b["base_type"]:
                    continue
                in_pk_a = col_key in table_a["pk_candidates"]
                in_pk_b = col_key in table_b["pk_candidates"]
                if in_pk_a and not in_pk_b:
                    _record_pair(
                        table_b_name,
                        table_a_name,
                        meta_b["names"][0],
                        meta_a["names"][0],
                    )
                elif in_pk_b and not in_pk_a:
                    _record_pair(
                        table_a_name,
                        table_b_name,
                        meta_a["names"][0],
                        meta_b["names"][0],
                    )
                elif in_pk_a and in_pk_b:
                    pk_count_a = len(table_a["pk_candidates"])
                    pk_count_b = len(table_b["pk_candidates"])
                    if pk_count_a >= 2 and pk_count_b == 1:
                        _record_pair(
                            table_a_name,
                            table_b_name,
                            meta_a["names"][0],
                            meta_b["names"][0],
                        )
                    elif pk_count_b >= 2 and pk_count_a == 1:
                        _record_pair(
                            table_b_name,
                            table_a_name,
                            meta_b["names"][0],
                            meta_a["names"][0],
                        )

            # Enhanced suffix-based matches with name similarity
            # Pattern 1: Direct suffix match (e.g. order_date_id -> date_id)
            for pk_norm, pk_cols in table_a["pk_candidates"].items():
                pk_meta = table_a["columns"].get(pk_norm)
                if not pk_meta:
                    continue
                for norm_b, meta_b in table_b["columns"].items():
                    if meta_b["base_type"] != pk_meta["base_type"]:
                        continue
                    if norm_b == pk_norm:
                        continue
                    
                    # Direct suffix match
                    if norm_b.endswith(pk_norm):
                        _record_pair(
                            table_b_name,
                            table_a_name,
                            meta_b["names"][0],
                            pk_cols[0],
                        )
                        continue
                    
                    # Enhanced: Check if column looks like a foreign key to this table
                    if _looks_like_foreign_key(table_b_name, table_a_name, meta_b["names"][0]):
                        # Additional check: name similarity
                        similarity = _name_similarity(norm_b, pk_norm)
                        if similarity >= 0.6:  # Configurable threshold
                            _record_pair(
                                table_b_name,
                                table_a_name,
                                meta_b["names"][0],
                                pk_cols[0],
                            )

            # Pattern 2: Reverse direction
            for pk_norm, pk_cols in table_b["pk_candidates"].items():
                pk_meta = table_b["columns"].get(pk_norm)
                if not pk_meta:
                    continue
                for norm_a, meta_a in table_a["columns"].items():
                    if meta_a["base_type"] != pk_meta["base_type"]:
                        continue
                    if norm_a == pk_norm:
                        continue
                    
                    # Direct suffix match
                    if norm_a.endswith(pk_norm):
                        _record_pair(
                            table_a_name,
                            table_b_name,
                            meta_a["names"][0],
                            pk_cols[0],
                        )
                        continue
                    
                    # Enhanced: Check if column looks like a foreign key to this table
                    if _looks_like_foreign_key(table_a_name, table_b_name, meta_a["names"][0]):
                        # Additional check: name similarity
                        similarity = _name_similarity(norm_a, pk_norm)
                        if similarity >= 0.6:  # Configurable threshold
                            _record_pair(
                                table_a_name,
                                table_b_name,
                                meta_a["names"][0],
                                pk_cols[0],
                            )

    # Build relationships with inferred cardinality
    for (left_table, right_table), column_pairs in pairs.items():
        # Infer cardinality based on available metadata
        left_meta = metadata[left_table]
        right_meta = metadata[right_table]
        
        # Determine if tables have primary keys in the relationship
        left_has_pk = any(
            col_name in [pair[0] for pair in column_pairs]
            for pk_list in left_meta["pk_candidates"].values()
            for col_name in pk_list
        )
        right_has_pk = any(
            col_name in [pair[1] for pair in column_pairs]
            for pk_list in right_meta["pk_candidates"].values()
            for col_name in pk_list
        )
        
        # Get sample values for cardinality inference
        left_values = []
        right_values = []
        if column_pairs:
            first_pair = column_pairs[0]
            left_col_key = _sanitize_identifier_name(
                first_pair[0],
                prefixes_to_drop=global_prefixes | _table_prefixes(left_table)
            )
            right_col_key = _sanitize_identifier_name(
                first_pair[1],
                prefixes_to_drop=global_prefixes | _table_prefixes(right_table)
            )
            
            if left_col_key in left_meta["columns"]:
                left_values = left_meta["columns"][left_col_key].get("values") or []
            if right_col_key in right_meta["columns"]:
                right_values = right_meta["columns"][right_col_key].get("values") or []
        
        # Infer cardinality (returns tuple like ("*", "1"))
        left_card, right_card = _infer_cardinality(
            left_values,
            right_values,
            left_has_pk,
            right_has_pk,
        )
        
        # Determine if SQL null probe should be executed for stricter inference
        strict_fk_detected = False
        if strict_join_inference and session:
            left_fqn_parts = left_meta.get("fqn")
            if isinstance(left_fqn_parts, data_types.FQNParts):
                strict_fk_detected = any(
                    _column_has_null_via_query(
                        session,
                        left_fqn_parts,
                        left_column,
                        null_check_cache,
                    )
                    for left_column, _ in column_pairs
                )

        # Infer join type based on relationship characteristics
        join_type = _infer_join_type(
            left_table,
            right_table,
            left_card,
            right_card,
            left_has_pk,
            right_has_pk,
            left_values,
            right_values,
            has_null_fk=strict_fk_detected,
        )
        
        # Log cardinality inference decision for debugging
        sample_info = f"samples: L={len(left_values)}, R={len(right_values)}"
        pk_info = f"PKs: L={left_has_pk}, R={right_has_pk}"
        join_type_name = "INNER" if join_type == semantic_model_pb2.JoinType.inner else "LEFT_OUTER"
        logger.debug(
            f"Relationship inference for {left_table} -> {right_table}: "
            f"{left_card}:{right_card}, JOIN={join_type_name} ({sample_info}, {pk_info})"
        )
        
        # Determine relationship type based on cardinality
        if left_card == "1" and right_card == "1":
            rel_type = semantic_model_pb2.RelationshipType.one_to_one
        elif left_card in ("*", "+") and right_card == "1":
            rel_type = semantic_model_pb2.RelationshipType.many_to_one
        elif left_card == "1" and right_card in ("*", "+"):
            rel_type = semantic_model_pb2.RelationshipType.one_to_many
        else:
            # Default to many_to_one for backward compatibility
            rel_type = semantic_model_pb2.RelationshipType.many_to_one
        
        relationship = semantic_model_pb2.Relationship(
            name=f"{left_table}_to_{right_table}",
            left_table=left_table,
            right_table=right_table,
            join_type=join_type,  # Use inferred join type instead of hardcoded inner
            relationship_type=rel_type,
        )
        for left_column, right_column in column_pairs:
            relationship.relationship_columns.append(
                semantic_model_pb2.RelationKey(
                    left_column=left_column, right_column=right_column
                )
            )
        relationships.append(relationship)
    
    logger.info(f"Inferred {len(relationships)} relationships across {len(raw_tables)} tables")
    return relationships


def _raw_table_to_semantic_context_table(
    database: str, schema: str, raw_table: data_types.Table
) -> semantic_model_pb2.Table:
    """
    Converts a raw table representation to a semantic model table in protobuf format.

    Args:
        database (str): The name of the database containing the table.
        schema (str): The name of the schema containing the table.
        raw_table (data_types.Table): The raw table object to be transformed.

    Returns:
        semantic_model_pb2.Table: A protobuf representation of the semantic table.

    This function categorizes table columns into TimeDimensions, Dimensions, or Measures based on their data type,
    populates them with sample values, and sets placeholders for descriptions and filters.
    """

    # For each column, decide if it is a TimeDimension, Measure, or Dimension column.
    # For now, we decide this based on datatype.
    # Any time datatype, is TimeDimension.
    # Any varchar/text is Dimension.
    # Any numerical column is Measure.

    time_dimensions = []
    dimensions = []
    facts: List[semantic_model_pb2.Fact] = []
    used_identifier_names: set[str] = set()
    table_prefixes = _table_prefixes(raw_table.name)

    for col in raw_table.columns:
        base_type = _base_type_from_type(col.column_type)
        if _is_time_like_column(col):
            time_data_type = col.column_type
            if time_data_type.split("(")[0].upper() in {"STRING", "VARCHAR", "TEXT", "CHAR", "CHARACTER", "NVARCHAR"}:
                time_data_type = "TIMESTAMP_NTZ"
            time_dimension_name = _safe_semantic_identifier(
                col.column_name,
                used_identifier_names,
                "time_dimension",
                prefixes_to_drop=table_prefixes,
            )
            time_dimensions.append(
                semantic_model_pb2.TimeDimension(
                    name=time_dimension_name,
                    expr=col.column_name,
                    data_type=time_data_type,
                    sample_values=col.values,
                    synonyms=[_PLACEHOLDER_COMMENT],
                    description=col.comment if col.comment else _PLACEHOLDER_COMMENT,
                )
            )

        elif base_type in DIMENSION_DATATYPES:
            dimension_name = _safe_semantic_identifier(
                col.column_name,
                used_identifier_names,
                "dimension",
                prefixes_to_drop=table_prefixes,
            )
            dimensions.append(
                semantic_model_pb2.Dimension(
                    name=dimension_name,
                    expr=col.column_name,
                    data_type=col.column_type,
                    sample_values=col.values,
                    synonyms=[_PLACEHOLDER_COMMENT],
                    description=col.comment if col.comment else _PLACEHOLDER_COMMENT,
                )
            )

        elif base_type in MEASURE_DATATYPES:
            if _is_identifier_like(col.column_name, base_type):
                identifier_dimension_name = _safe_semantic_identifier(
                    col.column_name,
                    used_identifier_names,
                    "dimension",
                    prefixes_to_drop=table_prefixes,
                )
                dimensions.append(
                    semantic_model_pb2.Dimension(
                        name=identifier_dimension_name,
                        expr=col.column_name,
                        data_type=col.column_type,
                        sample_values=col.values,
                        synonyms=[_PLACEHOLDER_COMMENT],
                        description=col.comment if col.comment else _PLACEHOLDER_COMMENT,
                    )
                )
                continue
            fact_name = _safe_semantic_identifier(
                col.column_name,
                used_identifier_names,
                "fact",
                prefixes_to_drop=table_prefixes,
            )
            facts.append(
                semantic_model_pb2.Fact(
                    name=fact_name,
                    expr=col.column_name,
                    data_type=col.column_type,
                    sample_values=col.values,
                    synonyms=[_PLACEHOLDER_COMMENT],
                    description=col.comment if col.comment else _PLACEHOLDER_COMMENT,
                )
            )
        elif base_type in OBJECT_DATATYPES:
            logger.warning(
                f"""We don't currently support {col.column_type} as an input column datatype to the Semantic Model. We are skipping column {col.column_name} for now."""
            )
            continue
        else:
            fallback_dimension_name = _safe_semantic_identifier(
                col.column_name,
                used_identifier_names,
                "dimension",
                prefixes_to_drop=table_prefixes,
            )
            logger.warning(
                f"Column datatype does not map to a known datatype. Input was = {col.column_type}. We are going to place as a Dimension for now."
            )
            dimensions.append(
                semantic_model_pb2.Dimension(
                    name=fallback_dimension_name,
                    expr=col.column_name,
                    data_type=col.column_type,
                    sample_values=col.values,
                    synonyms=[_PLACEHOLDER_COMMENT],
                    description=col.comment if col.comment else _PLACEHOLDER_COMMENT,
                )
            )
    if len(time_dimensions) + len(dimensions) + len(facts) == 0:
        raise ValueError(
            f"No valid columns found for table {raw_table.name}. Please verify that this table contains column's datatypes not in {OBJECT_DATATYPES}."
        )

    filters = _suggest_filters(raw_table)

    return semantic_model_pb2.Table(
        name=raw_table.name,
        base_table=semantic_model_pb2.FullyQualifiedTable(
            database=database, schema=schema, table=raw_table.name
        ),
        # For fields we can not automatically infer, leave a comment for the user to fill out.
        description=raw_table.comment if raw_table.comment else _PLACEHOLDER_COMMENT,
        filters=filters,
        dimensions=dimensions,
        time_dimensions=time_dimensions,
        facts=facts,
    )


def raw_schema_to_semantic_context(
    base_tables: List[str],
    semantic_model_name: str,
    conn: Session,
    n_sample_values: int = _DEFAULT_N_SAMPLE_VALUES_PER_COL,
    allow_joins: Optional[bool] = True,
    strict_join_inference: bool = False,
    llm_custom_prompt: str = "",
    enrich_with_llm: bool = False,
    progress_callback: Optional[Callable[[str], None]] = None,
) -> semantic_model_pb2.SemanticModel:
    """
    Converts a list of fully qualified ClickZetta table names into a semantic model.

    Parameters:
    - base_tables  (list[str]): Fully qualified table names to include in the semantic model.
    - semantic_model_name (str): A meaningful semantic model name.
    - conn (Session): ClickZetta session to reuse.
    - n_sample_values (int): The number of sample values per col.
    - allow_joins (bool | None): Whether to infer table relationships.
    - strict_join_inference (bool): If True, runs additional `IS NULL` probes per relationship to tighten join-type detection (requires extra SQL queries).
    - llm_custom_prompt (str): Optional user instructions forwarded to the DashScope enrichment step.

    Returns:
    - The semantic model (semantic_model_pb2.SemanticModel).

    This function fetches metadata for the specified tables, performs schema validation, extracts key information,
    enriches metadata from ClickZetta, and constructs a semantic model in protobuf format.
    It handles different workspaces and schemas within the same account by reusing the provided session.

    Raises:
    - AssertionError: If no valid tables are found in the specified schema.
    """

    # For FQN tables, the connector handles cross-schema access automatically.
    def _notify(message: str) -> None:
        if progress_callback:
            try:
                progress_callback(message)
            except Exception:
                logger.debug("Progress callback failed for message: {}", message)

    table_objects = []
    raw_tables_metadata: List[tuple[data_types.FQNParts, data_types.Table]] = []
    unique_database_schema: List[str] = []
    for table in base_tables:
        # Verify this is a valid FQN table. For now, we check that the table follows the following format.
        # {database}.{schema}.{table}
        fqn_table = create_fqn_table(table)
        fqn_databse_schema = f"{fqn_table.database}.{fqn_table.schema_name}"

        if fqn_databse_schema not in unique_database_schema:
            unique_database_schema.append(fqn_databse_schema)

        logger.info(f"Pulling column information from {fqn_table}")
        _notify(f"Fetching metadata for {fqn_table.database}.{fqn_table.schema_name}.{fqn_table.table}...")
        valid_schemas_tables_columns_df = get_valid_schemas_tables_columns_df(
            session=conn,
            workspace=fqn_table.database,
            table_schema=fqn_table.schema_name,
            table_names=[fqn_table.table],
        )
        if valid_schemas_tables_columns_df.empty:
            raise ValueError(
                (
                    "Unable to retrieve column metadata for table "
                    f"{fqn_table.database}.{fqn_table.schema_name}.{fqn_table.table}. "
                    "Shared or external catalogs (category=SHARED/EXTERNAL) do not expose information_schema views; "
                    "please copy the data into a managed workspace or provide manual metadata."
                )
            )

        # get the valid columns for this table.
        valid_columns_df_this_table = valid_schemas_tables_columns_df[
            valid_schemas_tables_columns_df["TABLE_NAME"] == fqn_table.table
        ]

        raw_table = get_table_representation(
            session=conn,
            workspace=fqn_table.database,
            schema_name=fqn_table.schema_name,
            table_name=fqn_table.table,  # Non-qualified table name
            table_index=0,
            ndv_per_column=n_sample_values,  # number of sample values to pull per column.
            columns_df=valid_columns_df_this_table,
            max_workers=1,
        )
        table_object = _raw_table_to_semantic_context_table(
            database=fqn_table.database,
            schema=fqn_table.schema_name,
            raw_table=raw_table,
        )
        table_objects.append(table_object)
        raw_tables_metadata.append((fqn_table, raw_table))
    # TODO(jhilgart): Call cortex model to generate a semantically friendly name here.

    relationships: List[semantic_model_pb2.Relationship] = []
    if allow_joins:
        relationships = _infer_relationships(
            raw_tables_metadata,
            session=conn if strict_join_inference else None,
            strict_join_inference=strict_join_inference,
        )

    context = semantic_model_pb2.SemanticModel(
        name=semantic_model_name,
        tables=table_objects,
        relationships=relationships,
    )
    context.description = _PLACEHOLDER_COMMENT
    context.custom_instructions = _PLACEHOLDER_COMMENT

    if enrich_with_llm:
        settings = get_dashscope_settings()
        if settings:
            actual_model = "qwen-plus-latest"
            logger.info(
                "Running DashScope enrichment for semantic model '{}' using model '{}'",
                semantic_model_name,
                actual_model,
            )
            _notify("Running DashScope enrichment to enhance descriptions and metrics...")
            settings = DashscopeSettings(
                api_key=settings.api_key,
                model=actual_model,
                base_url=settings.base_url,
                temperature=settings.temperature,
                top_p=settings.top_p,
                max_output_tokens=settings.max_output_tokens,
                timeout_seconds=settings.timeout_seconds,
            )
            client = DashscopeClient(settings)
            enrich_semantic_model(
                context,
                raw_tables_metadata,
                client,
                placeholder=_PLACEHOLDER_COMMENT,
                custom_prompt=llm_custom_prompt,
                session=conn,
            )
            _notify("DashScope enrichment complete.")
        else:
            logger.warning("LLM enrichment was requested but DashScope is not configured; skipping enrichment.")
            _notify("DashScope configuration missing; skipped enrichment.")
    return context


def append_comment_to_placeholders(yaml_str: str) -> str:
    """
    Finds all instances of a specified placeholder in a YAML string and appends a given text to these placeholders.
    This is the homework to fill out after your yaml is generated.

    Parameters:
    - yaml_str (str): The YAML string to process.

    Returns:
    - str: The modified YAML string with appended text to placeholders.
    """
    updated_yaml = []
    # Split the string into lines to process each line individually
    lines = yaml_str.split("\n")

    for line in lines:
        # Check if the placeholder is in the current line.
        # Strip the last quote to match.
        if line.rstrip("'").endswith(_PLACEHOLDER_COMMENT):
            # Replace the _PLACEHOLDER_COMMENT with itself plus the append_text
            updated_line = line + _FILL_OUT_TOKEN
            updated_yaml.append(updated_line)
        elif line.rstrip("'").endswith(AUTOGEN_TOKEN):
            updated_line = line + _AUTOGEN_COMMENT_TOKEN
            updated_yaml.append(updated_line)
        # Add comments to specific fields in certain sections.
        elif line.lstrip().startswith("join_type"):
            updated_yaml.append(line)
        elif line.lstrip().startswith("relationship_type"):
            updated_yaml.append(line)
        else:
            updated_yaml.append(line)

    # Join the lines back together into a single string
    return "\n".join(updated_yaml)


def _to_snake_case(s: str) -> str:
    """
    Convert a string into snake case.

    Parameters:
    s (str): The string to convert.

    Returns:
    str: The snake case version of the string.
    """
    # Replace common delimiters with spaces
    s = s.replace("-", " ").replace("_", " ")

    words = s.split(" ")

    # Convert each word to lowercase and join with underscores
    snake_case_str = "_".join([word.lower() for word in words if word]).strip()

    return snake_case_str


def generate_base_semantic_model_from_clickzetta(
    base_tables: List[str],
    conn: Session,
    semantic_model_name: str,
    n_sample_values: int = _DEFAULT_N_SAMPLE_VALUES_PER_COL,
    output_yaml_path: Optional[str] = None,
) -> None:
    """
    Generates a base semantic context from specified ClickZetta tables and exports it to a YAML file.

    Parameters:
        base_tables : Fully qualified names of ClickZetta tables to include in the semantic context.
        conn: ClickZetta session to reuse.
        semantic_model_name: The human readable model name. This should be semantically meaningful to an organization.
        output_yaml_path: Path for the output YAML file. If None, defaults to 'semantic_model_generator/output_models/YYYYMMDDHHMMSS_<semantic_model_name>.yaml'.
        n_sample_values: The number of sample values to populate for all columns.

    Returns:
        None. Writes the semantic context to a YAML file.
    """
    formatted_datetime = datetime.now().strftime("%Y%m%d%H%M%S")
    if not output_yaml_path:
        file_name = f"{formatted_datetime}_{_to_snake_case(semantic_model_name)}.yaml"
        if os.path.exists("semantic_model_generator/output_models"):
            write_path = f"semantic_model_generator/output_models/{file_name}"
        else:
            write_path = f"./{file_name}"
    else:  # Assume user gives correct path.
        write_path = output_yaml_path

    yaml_str = generate_model_str_from_clickzetta(
        base_tables,
        n_sample_values=n_sample_values if n_sample_values > 0 else 1,
        semantic_model_name=semantic_model_name,
        conn=conn,
    )

    with open(write_path, "w") as f:
        # Clarify that the YAML was autogenerated and that placeholders should be filled out/deleted.
        f.write(_AUTOGEN_COMMENT_WARNING)
        f.write(yaml_str)

    logger.info(f"Semantic model saved to {write_path}")

    return None


def generate_model_str_from_clickzetta(
    base_tables: List[str],
    semantic_model_name: str,
    conn: Session,
    n_sample_values: int = _DEFAULT_N_SAMPLE_VALUES_PER_COL,
    allow_joins: Optional[bool] = True,
    strict_join_inference: bool = False,
    enrich_with_llm: bool = False,
    llm_custom_prompt: str = "",
    progress_callback: Optional[Callable[[str], None]] = None,
) -> str:
    """
    Generates a base semantic context from specified ClickZetta tables and returns the raw string.

    Parameters:
        base_tables : Fully qualified names of ClickZetta tables to include in the semantic context.
        semantic_model_name: The human readable model name. This should be semantically meaningful to an organization.
        conn: ClickZetta session to reuse.
        n_sample_values: The number of sample values to populate for all columns.
        allow_joins: Whether to allow joins in the semantic context.
        strict_join_inference: When True, executes additional null-probe SQL queries to improve join type accuracy.
        llm_custom_prompt: Optional user instructions forwarded to the DashScope enrichment step.
        progress_callback: Optional callable invoked with human-readable progress updates.

    Returns:
        str: The raw string of the semantic context.
    """
    def _notify(message: str) -> None:
        if progress_callback:
            try:
                progress_callback(message)
            except Exception:
                logger.debug("Progress callback failed for message: {}", message)

    table_list = ", ".join(base_tables)
    logger.info("Generating semantic model '{}' from tables: {}", semantic_model_name, table_list)
    _notify("Collecting metadata from ClickZetta tables...")

    context = raw_schema_to_semantic_context(
        base_tables,
        n_sample_values=n_sample_values if n_sample_values > 0 else 1,
        semantic_model_name=semantic_model_name,
        allow_joins=allow_joins,
        strict_join_inference=strict_join_inference,
        llm_custom_prompt=llm_custom_prompt,
        enrich_with_llm=enrich_with_llm,
        conn=conn,
        progress_callback=_notify,
    )
    _notify("Constructing semantic model structure...")
    # Validate the generated yaml is within context limits.
    # We just throw a warning here to allow users to update.
    validate_context_length(context)
    _notify("Validating semantic model context length...")

    _notify("Converting semantic model to YAML...")
    yaml_str = proto_utils.proto_to_yaml(context)
    # Once we have the yaml, update to include to # <FILL-OUT> tokens.
    yaml_str = append_comment_to_placeholders(yaml_str)

    _notify("Semantic model generation complete.")
    logger.info("Semantic model '{}' generated successfully.", semantic_model_name)
    return yaml_str
